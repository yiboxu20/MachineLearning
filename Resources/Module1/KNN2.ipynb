{"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/yiboxu20/MachineLearning/blob/main/Resources/Module1/KNN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"5aucjVxENzP6"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVGwVJ7hdThc","outputId":"aa6c2b2b-ab4b-4470-f669-3d56a804734d","executionInfo":{"status":"ok","timestamp":1759447135659,"user_tz":240,"elapsed":80,"user":{"displayName":"Yibo Xu","userId":"16745282916973799078"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}],"source":["%pylab inline\n","from IPython.display import Image\n","import numpy.linalg as LA"]},{"cell_type":"markdown","metadata":{"id":"mILwld4ae25j"},"source":["## Curse of Dimensionality\n","The main statistical problem with KNN classifiers is that they do not work well with high dimensional\n","inputs, due to the **curse of dimensionality**.\n","\n","The basic problem is that the volume of space grows exponentially fast with dimension, so you\n","might have to look quite far away in space to find your nearest neighbor.\n","\n","Also in our algorithm, each time we need to exhaust all training points to find the $k$-nearest neighbours. It is very expensive and inefficient. (this can be solved by using some anchor points and other various hashing methods.)\n","\n","So it is important to use a metric that only cares about a subset of the dimensions."]},{"cell_type":"markdown","metadata":{"id":"pmDtvNPMNR1l"},"source":["## Large margin nearest neighbor\n","\n","Learn a distance function (pseudo-metric) designed for $k$-NN of the type, **Mahalanobis metric**:\n","$$d_M(\\mathbf{x},\\mathbf{z})= \\sqrt{(\\mathbf{x}-\\mathbf{z})^\\top M (\\mathbf{x}-\\mathbf{z})},$$\n"," where $M$ is a PSD. Particularly, $\\mathrm{rank}(M)$ may be less than $d$.\n","\n"," The goal of the algorithm is to distinguish between two types of special data points:\n"," - **target neighbors** of $\\mathbf{x}^{(i)}$: $k$ different neighbors from the same class as $\\mathbf{x}^{(i)}$, selected before learning. Target neighbors are the samples that should become nearest neighbors under the learned Mahalanobis metric.\n","\n"," - **impostors** of $\\mathbf{x}^{(i)}$: neighbor samples $\\mathbf{x}^{\\ell}$ with $\\ell\\in \\mathcal{N}_k(\\mathbf{x}^{(i)})$ from a different class, i.e, $y^{(\\ell)}\\ne y^{(i)}$.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"id":"CvjVX0uOXMZh","outputId":"5e2caf71-6416-4349-db54-6c2388b0f376","executionInfo":{"status":"ok","timestamp":1759447135701,"user_tz":240,"elapsed":40,"user":{"displayName":"Yibo Xu","userId":"16745282916973799078"}}},"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://github.com/yiboxu20/MachineLearning/blob/main/Resources/images/Lmnn.png?raw=true\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":2}],"source":["Image(url='https://github.com/yiboxu20/MachineLearning/blob/main/Resources/images/Lmnn.png?raw=true', width=700)"]},{"cell_type":"markdown","metadata":{"id":"K6UaDbHxXwvI"},"source":["- The first optimization goal is achieved by minimizing the average distance between each data point $\\mathbf{x}^{(i)}$ and their target neighbors, i.e.\n"," $$\\sum_{i=1}^N \\sum_{j\\in \\mathcal{N}_k(\\mathbf{x}^{(i)})}  d_M^2(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})=\\sum_{i=1}^N\\sum_{j\\in \\mathcal{N}_k(\\mathbf{x}^{(i)}) }(\\mathbf{x}^{(i)}-\\mathbf{x}^{(j)})^\\top M (\\mathbf{x}^{(i)}-\\mathbf{x}^{(j)}) $$\n","\n","- The second optimization goal to make sure the impostors should be far away. Distance to impostors $\\mathbf{x}^{(\\ell)}$ are more than 1 unit further away than to target neighbors $\\mathbf{x}^{(j)}$:\n","\n","  $$d^2_M(\\mathbf{x}^{(i)} , \\mathbf{x}^{(\\ell)})-d^2_M(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})\\ge 1, \\forall i, j,\\ell\\in \\mathcal{N}_k(\\mathbf{x}^{(i)}), \\ell: y^{(\\ell)}\\ne y^{(i)}$$\n","   The margin of exactly one unit fixes the scale of the matrix $M$. This is hard margin constraint.\n"]},{"cell_type":"markdown","metadata":{"id":"frYipkK7ey25"},"source":["### Soft Margin\n","Introduce the slack variable $\\xi$ to allow violations of margin constraint, with $\\xi_{ij\\ell}\\ge0$\n"," $$d^2_M(\\mathbf{x}^{(i)} , \\mathbf{x}^{(\\ell)})-d^2_M(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})\\ge 1-\\xi_{ij\\ell}, \\forall i, j,\\ell\\in \\mathcal{N}_k(\\mathbf{x}^{(i)}), \\ell: y^{(\\ell)}\\ne y^{(i)}$$\n","\n","Use the hinge loss function to rewrite the optimization, and solve the following semidefinite programming (SDP):\n","$$\\min_{M\\in \\mathbb{R}^{d\\times d}, \\xi} \\sum_{i=1}^N \\sum_{j\\in \\mathcal{N}_k(\\mathbf{x}^{(i)})}d_M^2(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) + \\lambda \\sum_{i=1}^N \\sum_{j\\in \\mathcal{N}_k(\\mathbf{x}^{(i)})} \\sum_{\\ell: y^{(\\ell)}\\ne y^{(i)}} \\max\\{0, d^2_M(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})- d^2_M(\\mathbf{x}^{(i)} ,\\mathbf{x}^{(\\ell)}) + 1 \\}$$\n","subject to $M\\ge 0$\n","\n","- $\\lambda>0$ is again the regularization parameter.\n","\n","- Similarly, the slack variable $\\xi_{ij\\ell}\\ge 0$ measures the amount by which the margin inequality is violated.\n","\n","- However, it is more difficult optimization problem since it has an extra PSD constraint.\n","\n","- often perform dimensional reduction on data to make $d$ smaller and thus\n","to speed up learning.\n","\n","This problem can be solved by `scipy` package `minimize` function with `L-BFGS-B` method.\n"]},{"cell_type":"markdown","metadata":{"id":"RtNWPcFBGs9Q"},"source":["This demo code is in [KNN2_code](https://github.com/yiboxu20/MachineLearning/blob/main/Resources/Module1/KNN2_code.ipynb).  This is mainly from https://github.com/johny-c/pylmnn."]},{"cell_type":"markdown","metadata":{"id":"vGNorrGTlPGz"},"source":["## Summary\n","- Non-parameterized algorithm for classification (majority vote) and\n","regression (averaging).\n","\n","- Simple and intuitive: require no explicit training step or optimization\n","process.\n","\n","- Need storage of training data at inference time; memory intensive for\n","large-scale datasets.\n","\n","- Computationally expensive: $O(Nd)$ computations for search through all\n","training data at inference time. Some smart methods are possible to significantly decrease the computations.\n","\n","- A proper distance function can be learned via metric learning for a given\n","dataset.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}