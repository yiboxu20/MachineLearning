{"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/yiboxu20/MachineLearning/blob/main/Resources/Module1/SVM4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"4knLBXzQLmwa"}},{"cell_type":"markdown","metadata":{"id":"Tfey6gHazW7p"},"source":["## Multiclass SVMs\n","### One-against-all scheme:\n","- Construct $K$ binary SVMs with parameters $(\\mathbf{w}^j, w_0^j),1\\le j\\le K$. Each classifiers the current class $j$ against all others.   \n","\n","- Given a new feature $\\mathbf{x}$, the further the point is from the decision boundary of some binary SVM in the \"positive\" direction, the more likely we think it belongs to that class.\n","\n","- That is, the predicted class is set to\n","  $$ \\text{arg}\\max_{1\\le j\\le K}\\{\\phi(\\mathbf{x})\\cdot(\\mathbf{w}^{j}) +w_0^j\\}$$\n","\n","### One-against-one scheme:\n","\n","- construct a binary SVMs for each pair of classes; in total $\\frac{K(K-1)}{2}$ binary SVMs for $K$-class classification.\n","\n","- All the binary classifiers are tested; for each of them, a win for one class is\n","a vote for that class. The class with the most votes wins.\n","\n","Example: MNIST classification"]},{"cell_type":"markdown","metadata":{"id":"0M6mR7rh2iNC"},"source":["# Summary of SVM and logistic regression: a unified framework\n","\n","- Construct a **decision function** (or **classifier**) $f_\\theta(\\mathbf{x}):\\mathbb{R}^d \\rightarrow \\mathbb{R}$, parameterized by $\\theta$.\n","  -  Linear model: $f_\\theta(\\mathbf{x})= \\mathbf{x}\\mathbf{w}+w_0$ with $\\theta=(\\mathbf{w}, w_0)$.\n","  - Polynomials model up to degree $2$: $f_\\theta(\\mathbf{x})=\\phi(\\mathbf{x})\\mathbf{w} +w_0$, with $\\phi(\\mathbf{x})=[x_1, \\dots, x_d, x_1^2, \\dots, x_d^2, \\sqrt{2}x_1x_2, \\dots, \\sqrt{2}x_{d-1}x_d]$ and $\\theta=(\\mathbf{w}, w_0)$.\n","\n","- **Decision boundary** is given by the equation $f_{\\theta}(\\mathbf{x})=0$\n","\n","- Magnitude of $f_\\theta(\\mathbf{x})$ or equivanlently, $yf_\\theta(\\mathbf{x})$ reflects how far the sample $\\mathbf{x}$ is from the decision boundary, where label $y\\in\\{\\pm1\\}$.\n","\n","- Sign of $f_\\theta(\\mathbf{x})$ predicts the class. Sign of $yf_\\theta(\\mathbf{x})$ indicates if sample $\\mathbf{x}$ is\n","correctly classified.\n","\n","- Choosing a decreasing **loss function** $\\ell: \\mathbb{R}\\rightarrow \\mathbb{R}$, penalizing upon the\n","discrepancy between model output and the corresponding label.\n","\n","- Inconsistency between the output $f_\\theta(\\mathbf{x})$ and $y$ is measured by $\\ell(f_\\theta(\\mathbf{x}))$; negative $yf_\\theta(\\mathbf{x})$ implies misclassification and incurs a relatively large loss\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yX2IiIXBo1RN"},"source":["Fit the model on training dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$,\n","\n","$$\\boxed{\\min_{\\theta}\\lambda\\sum_{i=1}^N\\ell(y^{(i)}f_\\theta(\\mathbf{x}^{(i)}))+ R(\\theta) } $$\n","Often comes with with a regularizer $R(\\theta)$ like $\\|\\theta\\|_2^2, \\|\\theta\\|_1$."]},{"cell_type":"markdown","metadata":{"id":"CpdzzxfKuzkz"},"source":["\n","\n","### 1.  Perceptron:\n","- Classifier: $f_\\theta(\\mathbf{x})= \\mathbf{x}\\mathbf{w} +w_0$.\n","\n","- Loss function: 0-1 loss function. $\\ell(z)=\\mathbb{1}_{z<0}$.\n","\n","- Regularizer: None.\n","\n","- Minimize the total number of misclassified training samples\n","\n","- Solver: SGD with the \"fake\" gradient\n","\n","### 2.   Logistic regression:\n","- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{x}\\mathbf{w} +w_0$.\n","- Loss function: log-loss function. $\\ell(z)=\\log(1+\\exp(-z))$. If $y\\in\\{\\pm 1\\}$, $p(y|\\mathbf{x},\\theta)=\\ell(yf_\\theta(\\mathbf{x}))$.\n","\n","- Regularizer: None.\n","\n","- Minimize the negative log likelihood when the probability is defined as before.\n","\n","- Solver: SGD with the true gradient\n","\n","### 3. Soft Margin Classification\n","- Classifier: $f_\\theta(\\mathbf{x})= \\mathbf{x}\\mathbf{w} +w_0$.\n","\n","- Loss function: Hinge loss function. $\\ell(z)=\\max\\{1-z, 0\\}$.\n","\n","- Regularizer: Tikhonov($l_2$) regulation, $\\|\\mathbf{w}\\|_2^2$\n","\n","- Minimize the hinge loss with Tikhonov regularization.\n","\n","- Solver: solve the primal problem directily with SGD or solve the dual problem with SMO.\n","\n","### 4. Hard Margin Classification\n","- Same as Soft Margin Classification with $\\lambda =+\\infty$.\n","\n","- Solver: solve the dual problem with SMO.\n","\n","### 5. Kernel SVM\n","- Classifier: $f_\\theta(\\mathbf{x})= \\phi(\\mathbf{x})\\mathbf{w} +w_0=\\sum_{i=1}^N\\alpha_iy^{(i)}\\mathcal{K}(\\mathbf{x}^{(i)}, \\mathbf{x})+w_0$.\n","\n","- Loss function: Hinge loss function. $\\ell(z)=\\max\\{1-z, 0\\}$.\n","\n","- Regularizer: Tikhonov($l_2$) regulation, $\\|\\mathbf{w}\\|_2^2$.\n","\n","- Minimize the hinge loss with Tikhonov regularization.\n","\n","- Solver: solve the dual problem with SMO.\n","\n","  - Polynomial kernels: $\\mathcal{K}(\\mathbf{x}, \\mathbf{z}) = (1+\\mathbf{x}^\\top\\mathbf{z})^k $.\n","\n","  - Gaussian kernels: $\\mathcal{K}(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\|\\mathbf{x}-\\mathbf{z}\\|^2/2\\sigma^2)$.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6dxZCFy5TnCs","outputId":"cdfde62d-5e4a-4931-eab5-87670d228450","executionInfo":{"status":"ok","timestamp":1759446636277,"user_tz":240,"elapsed":90,"user":{"displayName":"Yibo Xu","userId":"16745282916973799078"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}],"source":["%pylab inline"]},{"cell_type":"markdown","metadata":{"id":"G--iSgX-UB4J"},"source":["<img src=\"https://github.com/yiboxu20/MachineLearning/blob/main/Resources/images/losses.png?raw=true\" width=\"500\" />\n","\n","1. log loss. $\\ell(z)=\\log(1+\\exp(-z))$\n","2. exp loss. $\\ell(z)=\\exp(-z)$\n","\n","They are both smooth convex relaxation of 0-1 loss function."]},{"cell_type":"markdown","metadata":{"id":"4vbjH90_WBKu"},"source":["## SVM for regression\n","Fit the model on training dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, in regression, we have\n","\n","$$\\boxed{\\min_{\\theta}\\lambda\\sum_{i=1}^N\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w}))+ R(\\theta) } $$\n","where $f(\\mathbf{x}^{(i)},\\mathbf{w})$ is the function for regression, i.e, $f(\\mathbf{x}^{(i)},\\mathbf{w})= \\phi(\\mathbf{x})\\mathbf{w}$. $\\phi(\\mathbf{x})$ can be up to $k$-th order polynomials, or even Gaussian basis functions\n","$$\\phi(\\mathbf{x})=\\left[\\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{x}^{(1)}\\|^2}{2\\sigma^2}\\right), \\dots, \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{x}^{(N)}\\|^2}{2\\sigma^2}\\right) \\right]\\in \\mathbb{R}^N $$\n","\n","### 1. ridge regression\n","\n","- Loss function: Square loss. $\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w})) =\\left(y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w})\\right)^2$.\n","\n","- Regularizer: Tikhonov regularization. $\\|\\mathbf{w}\\|^2_2$\n","\n","### 2. LASSO\n","\n","- Loss function: Square loss. $\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w})) =\\left(y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w})\\right)^2$.\n","\n","- Regularizer: $l_1$ regularization. $\\|\\mathbf{w}\\|_1$\n","\n","### 3.  $\\epsilon$-insensitive loss\n","\n","- Loss function: **$\\epsilon$-insensitive loss function**:\n","$$\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w}))=\\max(|y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w})|-\\epsilon , 0) $$\n","\n","- Regularizer: Tikhonov regularization. $\\|\\mathbf{w}\\|^2_2$\n","\n","### 4. Huber loss\n","- Loss function: Huber loss, $\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w}))=h(y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w}))$\n","$$h(r) = \\begin{cases} r^2 & \\text{if }|r|\\le c  \\\\ 2c|r|-c^2 & \\text{Otherwise} \\end{cases} $$\n","\n","- Regularizer: Tikhonov regularization. $\\|\\mathbf{w}\\|^2_2$\n","\n","- (mixed quadratic/linear): robustness to outliers\n","\n","\n","\n","<img src=\"https://github.com/yiboxu20/MachineLearning/blob/main/Resources/images/loss_function.png?raw=true\" width=\"500\" />"]},{"cell_type":"markdown","metadata":{"id":"RPj410D3AdEL"},"source":["### Final notes on Loss functions\n","Regressors and classifiers can be constructed by a “mix ‘n’ match” of loss\n","functions and regularizers to obtain a learning machine suited to a\n","particular application.\n","\n","- $l_1$—SVM\n","$$ \\min_{\\mathbf{w}\\in \\mathbb{R}^d, w_0\\in \\mathbb{R}} \\lambda\\sum_{i=1}^N \\max\\left\\{0, 1-\\mathbf{y}^{(i)}(\\mathbf{x}^{(i)}\\mathbf{w} +w_0)\\right\\} +\\frac{1}{2} \\|\\mathbf{w}\\|_1$$\n","- Least squares SVM\n","\n","$$ \\min_{\\mathbf{w}\\in \\mathbb{R}^d, w_0\\in \\mathbb{R}} \\lambda\\sum_{i=1}^N \\left(\\max\\left\\{0, 1-\\mathbf{y}^{(i)}(\\mathbf{x}^{(i)}\\mathbf{w} +w_0)\\right\\}\\right)^2 +\\frac{1}{2} \\|\\mathbf{w}\\|_2^2$$"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}